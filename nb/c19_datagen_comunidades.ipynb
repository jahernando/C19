{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C19 data preparation\n",
    "Prepares dataframes with C19 and meteorological data.\n",
    "- Meteorological data source: http://www.aemet.es/es/datos_abiertos/AEMET_OpenData\n",
    "- C19 data source: https://github.com/datadista/datasets/tree/master/COVID%2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an API key here: https://opendata.aemet.es/centrodedescargas/altaUsuario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/data_Andalucia.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2fadd9f377ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msensor_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdf_comunidades\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/data_{}.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mdf_comunidades\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unnamed: 0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdf_comunidades\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fecha'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_comunidades\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fecha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%Y-%m-%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/IC-3.7-2018-11-14/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/IC-3.7-2018-11-14/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/IC-3.7-2018-11-14/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/IC-3.7-2018-11-14/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/IC-3.7-2018-11-14/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/data_Andalucia.csv' does not exist"
     ]
    }
   ],
   "source": [
    "api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the COVID data\n",
    "Source: https://github.com/datadista/datasets/tree/master/COVID%2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/datadista/datasets/master/COVID%2019/ccaa_covid19_casos.csv\n",
    "! wget https://raw.githubusercontent.com/datadista/datasets/master/COVID%2019/ccaa_covid19_fallecidos.csv\n",
    "! wget https://raw.githubusercontent.com/datadista/datasets/master/COVID%2019/ccaa_covid19_uci.csv\n",
    "! wget https://raw.githubusercontent.com/datadista/datasets/master/COVID%2019/ccaa_covid19_hospitalizados.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define main frunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weather dataframe for the specified station and date range.\n",
    "def get_meteo_df(station,date_init,date_final,api_key):\n",
    "\n",
    "    # Send the initial request.\n",
    "    conn = http.client.HTTPSConnection(\"opendata.aemet.es\")\n",
    "    request_str = \"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/{}/fechafin/{}/estacion/{}/?api_key={}\".format(date_init,date_final,station,api_key)\n",
    "    headers = {'cache-control': \"no-cache\"}\n",
    "    conn.request(\"GET\", request_str, headers=headers)\n",
    "\n",
    "    # Interpret the response.\n",
    "    res_init = conn.getresponse()\n",
    "    data_init = res_init.read()\n",
    "    dict_init = ast.literal_eval(data_init.decode(\"utf-8\"))\n",
    "    url_init = dict_init['datos']\n",
    "    url_meta = dict_init['metadatos']\n",
    "\n",
    "    # Send the request for the metadata.\n",
    "    #print(\"Requesting metadata from:\",url_meta)\n",
    "    conn.request(\"GET\", url_meta, headers=headers)\n",
    "\n",
    "    res_meta = conn.getresponse()\n",
    "    data_meta = res_meta.read()\n",
    "    dict_meta = data_meta.decode(\"ISO-8859-1\")\n",
    "    #print(dict_meta)\n",
    "\n",
    "    # Send the request for the data.\n",
    "    #print(\"Requesting data from:\",url_init)\n",
    "    conn.request(\"GET\", url_init, headers=headers)\n",
    "\n",
    "    # Interpret the response.\n",
    "    res_final = conn.getresponse()\n",
    "    data_final = res_final.read()\n",
    "    dict_data = ast.literal_eval(data_final.decode(\"ISO-8859-1\"))\n",
    "    \n",
    "    return pd.DataFrame(dict_data)\n",
    "\n",
    "def prepare_df(df):\n",
    "    \n",
    "    # Check that all required keys exist in the dataframe.\n",
    "    required_keys = ['fecha', 'prec', 'sol', 'tmax', 'tmed', 'tmin']\n",
    "    for rk in required_keys:\n",
    "        if(not (rk in df)): \n",
    "            print(\"Warning: dataframe missing\",rk)\n",
    "            return None\n",
    "        \n",
    "    # Extract required elements.\n",
    "    meteo = df[required_keys].copy()\n",
    "    \n",
    "    # Replace comma with dot.\n",
    "    meteo[['prec', 'sol', 'tmax', 'tmed', 'tmin']] = meteo[['prec', 'sol', 'tmax', 'tmed', 'tmin']].apply(lambda x: x.str.replace(',','.'))\n",
    "    \n",
    "    # Replace Ip with 0.0.\n",
    "    meteo[['prec']] = meteo[['prec']].apply(lambda x: x.str.replace('Ip','0.0'))\n",
    "    \n",
    "    # Convert to numerical values.\n",
    "    meteo[['prec','sol','tmax','tmed','tmin']] = meteo[['prec','sol','tmax','tmed','tmin']].astype('float')\n",
    "\n",
    "    # Convert dates to datetime objects.\n",
    "    meteo['fecha'] = pd.to_datetime(meteo['fecha'], format=\"%Y-%m-%d\")\n",
    "    \n",
    "    return meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary associating a weather sensor to each region.\n",
    "sensor_dict = {\n",
    "    \"Andalucia\"         : \"5402\" , # CORDOBA/AEROPUERTO\n",
    "    \"Aragon\"            : \"9434\" , # ZARAGOZA/AEROPUERTO\n",
    "    \"Asturias\"          : \"1208H\", # GIJON, MUSEL\n",
    "    \"Baleares\"          : \"B278\" , # PALMA DE MALLORCA/SON SAN JUAN\n",
    "    \"Canarias\"          : \"C029O\", # LANZAROTE/AEROPUERTO\n",
    "    \"Cantabria\"         : \"1111\" , # SANTANDER I,CMT\n",
    "    \"Castilla-La Mancha\": \"4121\" , # CIUDAD REAL\n",
    "    \"Castilla y Leon\"   : \"2422\" , # VALLADOLID\n",
    "    \"Cataluna\"          : \"0016A\", # REUS/AEROPUERTO\n",
    "    \"Ceuta\"             : \"5000C\", # CEUTA\n",
    "    \"C. Valenciana\"     : \"8414A\", # VALENCIA/AEROPUERTO\n",
    "    \"Extremadura\"       : \"3469A\", # CACERES\n",
    "    \"Galicia\"           : \"1428\" , # SANTIAGO DE COMPOSTELA/LABACOLLA\n",
    "    \"Madrid\"            : \"3200\" , # MADRID/GETAFE\n",
    "    \"Melilla\"           : \"6000A\", # MELILLA\n",
    "    \"Murcia\"            : \"7178I\", # MURCIA\n",
    "    \"Navarra\"           : \"9263D\", # PAMPLONA/NOAIN\n",
    "    \"Pais Vasco\"        : \"1024E\", # SAN SEBASTIAN,IGUELDO\n",
    "    \"La Rioja\"          : \"9170\"   # LOGRONO/AGONCILLO\n",
    "}\n",
    "meteo_regions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch a dataframe for each region over the selected date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_init = \"2020-02-27T00:00:00UTC\"\n",
    "date_final = \"2020-03-30T23:59:59UTC\"\n",
    "for region,station in sensor_dict.items():\n",
    "    print(region,station)\n",
    "    df = get_meteo_df(station,date_init,date_final,api_key)\n",
    "    meteo = prepare_df(df)\n",
    "    meteo_regions[region] = meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the COVID data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data.\n",
    "cases = pd.read_csv(\"ccaa_covid19_casos.csv\")\n",
    "ucases = pd.read_csv(\"ccaa_covid19_uci.csv\")\n",
    "fcases = pd.read_csv(\"ccaa_covid19_fallecidos.csv\")\n",
    "hcases = pd.read_csv(\"ccaa_covid19_hospitalizados.csv\")\n",
    "\n",
    "# Remove all accents from the region names.\n",
    "cases['CCAA'] = cases['CCAA'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "ucases['CCAA'] = ucases['CCAA'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "fcases['CCAA'] = fcases['CCAA'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "hcases['CCAA'] = hcases['CCAA'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "# Set the region name as index.\n",
    "cases  = cases.set_index('CCAA')\n",
    "ucases = ucases.set_index('CCAA')\n",
    "fcases = fcases.set_index('CCAA')\n",
    "hcases = hcases.set_index('CCAA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the COVID data into the dataframe for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions = {}\n",
    "for region,df in meteo_regions.items():\n",
    "    \n",
    "    print(region)\n",
    "    \n",
    "    # Get a new dataframe of cases with the dates and # of cases as columns.\n",
    "    cframe = pd.DataFrame({'ncases'        : cases.loc[region][1:].values, \n",
    "                           'fecha'         : cases.loc[region].keys()[1:].values})\n",
    "    uframe = pd.DataFrame({'uci'           : ucases.loc[region][1:].values, \n",
    "                           'fecha'         : ucases.loc[region].keys()[1:].values})\n",
    "    fframe = pd.DataFrame({'fallecidos'    : fcases.loc[region][1:].values, \n",
    "                           'fecha'         : fcases.loc[region].keys()[1:].values})\n",
    "    hframe = pd.DataFrame({'hospitalizados': hcases.loc[region][1:].values, \n",
    "                           'fecha'         : hcases.loc[region].keys()[1:].values})\n",
    "    \n",
    "    # Change the dates to datetime objects.\n",
    "    cframe['fecha'] = pd.to_datetime(cframe['fecha'], format=\"%Y-%m-%d\")\n",
    "    uframe['fecha'] = pd.to_datetime(uframe['fecha'], format=\"%Y-%m-%d\")\n",
    "    fframe['fecha'] = pd.to_datetime(fframe['fecha'], format=\"%Y-%m-%d\")\n",
    "    hframe['fecha'] = pd.to_datetime(hframe['fecha'], format=\"%Y-%m-%d\")\n",
    "    \n",
    "    # Merge the dataframes.\n",
    "    mdf = pd.merge(df,  cframe, on = 'fecha', how='outer')\n",
    "    mdf = pd.merge(mdf, uframe, on = 'fecha', how='outer')\n",
    "    mdf = pd.merge(mdf, fframe, on = 'fecha', how='outer')\n",
    "    mdf = pd.merge(mdf, hframe, on = 'fecha', how='outer')\n",
    "    df_regions[region] = mdf\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write all the dataframes to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.isdir(\"data\")):\n",
    "    os.mkdir(\"data\")\n",
    "for key, val in df_regions.items():\n",
    "    val.to_csv(\"data/data_{}.csv\".format(str(key)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! zip data_C19.zip data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of miscellaneous information.\n",
    "# Population data from: Cifras oficiales de población resultantes de la revisión del Padrón municipal a 1 de enero (year 2018)\n",
    "misc_dict = {\n",
    "    \"Andalucia\"         : {\"geoId\": \"AN\", \"countryterritoryCode\": \"AND\", \"popData2018\": 8384408},\n",
    "    \"Aragon\"            : {\"geoId\": \"AR\", \"countryterritoryCode\": \"ARA\", \"popData2018\": 1308728},\n",
    "    \"Asturias\"          : {\"geoId\": \"AS\", \"countryterritoryCode\": \"AST\", \"popData2018\": 1028244},\n",
    "    \"Baleares\"          : {\"geoId\": \"BA\", \"countryterritoryCode\": \"BAL\", \"popData2018\": 1128908},\n",
    "    \"Canarias\"          : {\"geoId\": \"CN\", \"countryterritoryCode\": \"CAN\", \"popData2018\": 2127685},\n",
    "    \"Cantabria\"         : {\"geoId\": \"CT\", \"countryterritoryCode\": \"CAB\", \"popData2018\": 580229},\n",
    "    \"Castilla-La Mancha\": {\"geoId\": \"CM\", \"countryterritoryCode\": \"CLM\", \"popData2018\": 2026807},\n",
    "    \"Castilla y Leon\"   : {\"geoId\": \"CL\", \"countryterritoryCode\": \"CYL\", \"popData2018\": 2409164},\n",
    "    \"Cataluna\"          : {\"geoId\": \"CA\", \"countryterritoryCode\": \"CAT\", \"popData2018\": 7600065},\n",
    "    \"Ceuta\"             : {\"geoId\": \"CE\", \"countryterritoryCode\": \"CEU\", \"popData2018\": 85144},\n",
    "    \"C. Valenciana\"     : {\"geoId\": \"CV\", \"countryterritoryCode\": \"CVA\", \"popData2018\": 4963703},\n",
    "    \"Extremadura\"       : {\"geoId\": \"EX\", \"countryterritoryCode\": \"EXT\", \"popData2018\": 1072863},\n",
    "    \"Galicia\"           : {\"geoId\": \"GA\", \"countryterritoryCode\": \"GAL\", \"popData2018\": 2701743},\n",
    "    \"Madrid\"            : {\"geoId\": \"MA\", \"countryterritoryCode\": \"MAD\", \"popData2018\": 6578079},\n",
    "    \"Melilla\"           : {\"geoId\": \"ME\", \"countryterritoryCode\": \"MEL\", \"popData2018\": 86384},\n",
    "    \"Murcia\"            : {\"geoId\": \"MU\", \"countryterritoryCode\": \"MUR\", \"popData2018\": 1478509},\n",
    "    \"Navarra\"           : {\"geoId\": \"NA\", \"countryterritoryCode\": \"NAV\", \"popData2018\": 647554},\n",
    "    \"Pais Vasco\"        : {\"geoId\": \"PV\", \"countryterritoryCode\": \"PVA\", \"popData2018\": 2199088},\n",
    "    \"La Rioja\"          : {\"geoId\": \"LR\", \"countryterritoryCode\": \"RIO\", \"popData2018\": 315675}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the dataframes.\n",
    "cdf = None\n",
    "for key in df_regions.keys():\n",
    "    \n",
    "    # Add the misc information to this dataframe.\n",
    "    cframe = df_regions[key]\n",
    "    cframe['countriesAndTerritories'] = key\n",
    "    cframe['geoId']                   = misc_dict[key]['geoId']\n",
    "    cframe['countryterritoryCode']    = misc_dict[key]['countryterritoryCode']\n",
    "    cframe['popData2018']             = misc_dict[key]['popData2018']\n",
    "    \n",
    "    if(cdf is None):\n",
    "        cdf = cframe\n",
    "    else:\n",
    "        cdf = cdf.append(cframe)\n",
    "\n",
    "# Reset the index count.\n",
    "cdf = cdf.reset_index()\n",
    "\n",
    "# Change column names.\n",
    "cdf = cdf.rename(columns={\"fecha\": \"dateRep\", \"ncases\": \"cases\", \"fallecidos\": \"deaths\", \"hospitalizados\": \"hospitalized\"})\n",
    "\n",
    "# Add columns for day, month, and year.\n",
    "cdf['day']   = cdf.apply(lambda row: row['dateRep'].date().day, axis=1)\n",
    "cdf['month'] = cdf.apply(lambda row: row['dateRep'].date().month, axis=1)\n",
    "cdf['year']  = cdf.apply(lambda row: row['dateRep'].date().year, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.to_csv(\"data_communities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for running quick tests of individual stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estacion = \"1109\"\n",
    "df = get_meteo_df(estacion,date_init,date_final,api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo = prepare_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = list(df_regions.keys())[6]\n",
    "df = df_regions[region]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(6.0)\n",
    "fig.set_figwidth(16.0)\n",
    "\n",
    "plt.plot(df.fecha,df['ncases'])\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "dloc = mdates.DayLocator()  # every month\n",
    "plt.gca().xaxis.set_major_locator(dloc)\n",
    "plt.ylabel('Total COVID cases')\n",
    "plt.title(\"Region: {}\".format(region))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE: Information for data request\n",
    "Get an API key here: https://opendata.aemet.es/centrodedescargas/altaUsuario?\n",
    "\n",
    "**Available stations (Valencia):**\n",
    "- 8058X: Oliva\n",
    "- 8325X: Polinyà de Xúquer\n",
    "- 8309X: Utiel (has full set of values)\n",
    "- 8416Y: Valencia\n",
    "- 8416: Valencia\n",
    "- 8414A: Valencia Aeropuerto (has full set of values)\n",
    "- 8293X: Xàtiva (has full set of values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estacion = \"8414A\"\n",
    "date_init = \"2020-01-01T00:00:00UTC\"\n",
    "date_final = \"2020-03-22T23:59:59UTC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the initial request.\n",
    "conn = http.client.HTTPSConnection(\"opendata.aemet.es\")\n",
    "request_str = \"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/{}/fechafin/{}/estacion/{}/?api_key={}\".format(date_init,date_final,estacion,api_key)\n",
    "headers = {'cache-control': \"no-cache\"}\n",
    "conn.request(\"GET\", request_str, headers=headers)\n",
    "\n",
    "# Interpret the response.\n",
    "res_init = conn.getresponse()\n",
    "data_init = res_init.read()\n",
    "dict_init = ast.literal_eval(data_init.decode(\"utf-8\"))\n",
    "url_init = dict_init['datos']\n",
    "url_meta = dict_init['metadatos']\n",
    "\n",
    "# Send the request for the metadata.\n",
    "print(\"Requesting metadata from:\",url_meta)\n",
    "conn.request(\"GET\", url_meta, headers=headers)\n",
    "\n",
    "res_meta = conn.getresponse()\n",
    "data_meta = res_meta.read()\n",
    "dict_meta = data_meta.decode(\"ISO-8859-1\")\n",
    "print(dict_meta)\n",
    "\n",
    "# Send the request for the data.\n",
    "print(\"Requesting data from:\",url_init)\n",
    "conn.request(\"GET\", url_init, headers=headers)\n",
    "\n",
    "# Interpret the response.\n",
    "res_final = conn.getresponse()\n",
    "data_final = res_final.read()\n",
    "dict_data = ast.literal_eval(data_final.decode(\"ISO-8859-1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo = pd.DataFrame(dict_data)\n",
    "meteo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use '.' as decimal separator (replace ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo[['prec', 'presMax', 'presMin', 'racha', 'sol', 'tmax', 'tmed', 'tmin', 'velmedia']] = meteo[['prec', 'presMax', 'presMin', 'racha', 'sol', 'tmax', 'tmed', 'tmin', 'velmedia']].apply(lambda x: x.str.replace(',','.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace 'Ip' precipitation values with '0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo[['prec']] = meteo[['prec']].apply(lambda x: x.str.replace('Ip','0.0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unwanted entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo.drop(['altitud','dir','horaPresMax','horaPresMin','horaracha','horatmax','horatmin','indicativo','nombre','provincia'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo[['prec','presMax','presMin','racha','sol','tmax','tmed','tmin','velmedia']] = meteo[['prec','presMax','presMin','racha','sol','tmax','tmed','tmin','velmedia']].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dates to datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo['fecha'] = pd.to_datetime(meteo['fecha'], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = meteo.columns.drop('fecha')\n",
    "fig,axs =  plt.subplots(len(cols_to_plot), 1, figsize=(20,20), sharex=True)\n",
    "fig.tight_layout()\n",
    "for i, column in enumerate(cols_to_plot):\n",
    "    axs[i].plot(meteo.fecha, meteo[column])\n",
    "    axs[i].set_ylabel(column)\n",
    "axs[i].set_xlabel('date');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(meteo.fecha,meteo['tmax'])\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "months = mdates.MonthLocator()  # every month\n",
    "plt.gca().xaxis.set_major_locator(months)\n",
    "plt.ylabel('Max Temperature (C)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
